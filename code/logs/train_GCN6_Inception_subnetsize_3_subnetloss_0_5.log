Using backend: pytorch
processId: 7658
prarent processId: 1
{
    "train_set": "../data/train_annotated.json",
    "dev_set": "../data/dev.json",
    "test_set": "../data/test.json",
    "train_set_save": "../data/prepro_data/train_BERT.pkl",
    "dev_set_save": "../data/prepro_data/dev_BERT.pkl",
    "test_set_save": "../data/prepro_data/test_BERT.pkl",
    "checkpoint_dir": "checkpoint",
    "fig_result_dir": "fig_result",
    "model_name": "GCN6_Inception_subnetsize_3_subnetloss_0_5",
    "pretrain_model": "",
    "vocabulary_size": 200000,
    "relation_nums": 97,
    "entity_type_num": 7,
    "max_entity_num": 80,
    "word_pad": 0,
    "entity_type_pad": 0,
    "entity_id_pad": 0,
    "word_emb_size": 10,
    "pre_train_word": false,
    "data_word_vec": null,
    "finetune_word": false,
    "use_entity_type": true,
    "entity_type_size": 20,
    "use_entity_id": true,
    "entity_id_size": 20,
    "nlayers": 1,
    "lstm_hidden_size": 32,
    "lstm_dropout": 0.1,
    "lr": 0.001,
    "batch_size": 5,
    "test_batch_size": 16,
    "epoch": 300,
    "test_epoch": 5,
    "weight_decay": 0.0001,
    "negativa_alpha": 4.0,
    "log_step": 20,
    "save_model_freq": 3,
    "mention_drop": false,
    "gcn_layers": 6,
    "gcn_dim": 808,
    "dropout": 0.6,
    "activation": "relu",
    "bert_hid_size": 768,
    "bert_path": "../PLM/bert-base-uncased",
    "bert_fix": false,
    "coslr": true,
    "clip": -1,
    "k_fold": "none",
    "use_model": "bert",
    "input_theta": -1,
    "subnetwork_loss_scale": 0.5
}
Reading data from ../data/train_annotated.json.
load preprocessed data from ../data/prepro_data/train_BERT.pkl.
Reading data from ../data/dev.json.
load preprocessed data from ../data/prepro_data/dev_BERT.pkl.
subnet_bank_size:  2424
self.bank_size:  5656
self.num_gcn_layers:  6
<bound method Module.parameters of GAIN_BERT(
  (activation): ReLU()
  (entity_type_emb): Embedding(7, 20, padding_idx=0)
  (entity_id_emb): Embedding(81, 20, padding_idx=0)
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (GCN_layers): ModuleList(
    (0): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (1): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (2): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (3): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (4): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (5): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
  )
  (dropout): Dropout(p=0.6, inplace=False)
  (predict): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
    (1): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
    (2): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
  )
  (edge_layer): ModuleList(
    (0): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
    (1): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
    (2): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
  )
  (path_info_mapping): ModuleList(
    (0): Linear(in_features=3232, out_features=3232, bias=True)
    (1): Linear(in_features=3232, out_features=3232, bias=True)
    (2): Linear(in_features=3232, out_features=3232, bias=True)
  )
  (attention): ModuleList(
    (0): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
    (1): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
    (2): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
  )
)>
total parameters: 432135662
2021-08-23 06:09:44.820167 training from scratch with lr 0.001
2021-08-23 06:09:57.170913 begin..
/opt/conda/envs/GAIN/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: From v0.5, DGLHeteroGraph is merged into DGLGraph. You can safely replace dgl.batch_hetero with dgl.batch
  return warnings.warn(message, category=category, stacklevel=1)
/opt/conda/envs/GAIN/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: From v0.5, DGLHeteroGraph is merged into DGLGraph. You can safely replace dgl.unbatch_hetero with dgl.unbatch
  return warnings.warn(message, category=category, stacklevel=1)
/opt/conda/envs/GAIN/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2021-08-23 06:10:37.631489 | epoch  1 | step   20 |  ms/b 2023.02 | train loss 60.384 | NA acc: 0.93 | not NA acc: 0.00  | tot acc: 0.74 
2021-08-23 06:11:14.749229 | epoch  1 | step   40 |  ms/b 1855.88 | train loss 7.444 | NA acc: 0.96 | not NA acc: 0.00  | tot acc: 0.78 
2021-08-23 06:11:53.045754 | epoch  1 | step   60 |  ms/b 1914.82 | train loss 6.807 | NA acc: 0.98 | not NA acc: 0.00  | tot acc: 0.78 
2021-08-23 06:12:31.930227 | epoch  1 | step   80 |  ms/b 1944.22 | train loss 6.843 | NA acc: 0.98 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:13:09.772378 | epoch  1 | step  100 |  ms/b 1892.10 | train loss 6.782 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:13:45.406031 | epoch  1 | step  120 |  ms/b 1781.68 | train loss 6.545 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:14:20.654277 | epoch  1 | step  140 |  ms/b 1762.41 | train loss 6.634 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:15:02.090365 | epoch  1 | step  160 |  ms/b 2071.80 | train loss 6.488 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:15:37.615776 | epoch  1 | step  180 |  ms/b 1776.26 | train loss 6.238 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:16:13.282141 | epoch  1 | step  200 |  ms/b 1783.31 | train loss 6.051 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:16:51.268702 | epoch  1 | step  220 |  ms/b 1899.32 | train loss 6.050 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:17:30.018728 | epoch  1 | step  240 |  ms/b 1937.49 | train loss 6.074 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:18:08.323318 | epoch  1 | step  260 |  ms/b 1915.22 | train loss 6.214 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:18:43.432830 | epoch  1 | step  280 |  ms/b 1755.47 | train loss 6.007 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:19:24.429380 | epoch  1 | step  300 |  ms/b 2049.82 | train loss 5.959 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:20:00.846148 | epoch  1 | step  320 |  ms/b 1820.83 | train loss 5.763 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:20:40.260081 | epoch  1 | step  340 |  ms/b 1970.69 | train loss 5.818 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:21:17.815354 | epoch  1 | step  360 |  ms/b 1877.76 | train loss 5.714 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:21:55.064688 | epoch  1 | step  380 |  ms/b 1862.46 | train loss 5.647 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:22:30.862514 | epoch  1 | step  400 |  ms/b 1789.88 | train loss 5.734 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:23:08.862905 | epoch  1 | step  420 |  ms/b 1900.01 | train loss 5.472 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
