Using backend: pytorch
processId: 8078
prarent processId: 1
{
    "train_set": "../data/train_annotated.json",
    "dev_set": "../data/dev.json",
    "test_set": "../data/test.json",
    "train_set_save": "../data/prepro_data/train_BERT.pkl",
    "dev_set_save": "../data/prepro_data/dev_BERT.pkl",
    "test_set_save": "../data/prepro_data/test_BERT.pkl",
    "checkpoint_dir": "checkpoint",
    "fig_result_dir": "fig_result",
    "model_name": "GCN6_Inception_subnetsize_3_subnetloss_0_9",
    "pretrain_model": "",
    "vocabulary_size": 200000,
    "relation_nums": 97,
    "entity_type_num": 7,
    "max_entity_num": 80,
    "word_pad": 0,
    "entity_type_pad": 0,
    "entity_id_pad": 0,
    "word_emb_size": 10,
    "pre_train_word": false,
    "data_word_vec": null,
    "finetune_word": false,
    "use_entity_type": true,
    "entity_type_size": 20,
    "use_entity_id": true,
    "entity_id_size": 20,
    "nlayers": 1,
    "lstm_hidden_size": 32,
    "lstm_dropout": 0.1,
    "lr": 0.001,
    "batch_size": 5,
    "test_batch_size": 16,
    "epoch": 300,
    "test_epoch": 5,
    "weight_decay": 0.0001,
    "negativa_alpha": 4.0,
    "log_step": 20,
    "save_model_freq": 3,
    "mention_drop": false,
    "gcn_layers": 6,
    "gcn_dim": 808,
    "dropout": 0.6,
    "activation": "relu",
    "bert_hid_size": 768,
    "bert_path": "../PLM/bert-base-uncased",
    "bert_fix": false,
    "coslr": true,
    "clip": -1,
    "k_fold": "none",
    "use_model": "bert",
    "input_theta": -1,
    "subnetwork_loss_scale": 0.9
}
Reading data from ../data/train_annotated.json.
load preprocessed data from ../data/prepro_data/train_BERT.pkl.
Reading data from ../data/dev.json.
load preprocessed data from ../data/prepro_data/dev_BERT.pkl.
subnet_bank_size:  2424
self.bank_size:  5656
self.num_gcn_layers:  6
<bound method Module.parameters of GAIN_BERT(
  (activation): ReLU()
  (entity_type_emb): Embedding(7, 20, padding_idx=0)
  (entity_id_emb): Embedding(81, 20, padding_idx=0)
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (GCN_layers): ModuleList(
    (0): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (1): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (2): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (3): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (4): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (5): RelGraphConvLayer(
      (activation): ReLU()
      (conv): HeteroGraphConv(
        (mods): ModuleDict(
          (intra): GraphConv(in=808, out=808, normalization=right, activation=None)
          (inter): GraphConv(in=808, out=808, normalization=right, activation=None)
          (global): GraphConv(in=808, out=808, normalization=right, activation=None)
        )
      )
      (dropout): Dropout(p=0.6, inplace=False)
    )
  )
  (dropout): Dropout(p=0.6, inplace=False)
  (predict): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
    (1): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
    (2): Sequential(
      (0): Linear(in_features=15352, out_features=4848, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.6, inplace=False)
      (3): Linear(in_features=4848, out_features=97, bias=True)
    )
  )
  (edge_layer): ModuleList(
    (0): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
    (1): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
    (2): RelEdgeLayer(
      (activation): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
      (mapping): Linear(in_features=1616, out_features=808, bias=True)
    )
  )
  (path_info_mapping): ModuleList(
    (0): Linear(in_features=3232, out_features=3232, bias=True)
    (1): Linear(in_features=3232, out_features=3232, bias=True)
    (2): Linear(in_features=3232, out_features=3232, bias=True)
  )
  (attention): ModuleList(
    (0): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
    (1): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
    (2): Attention(
      (W): Bilinear(in1_features=4848, in2_features=3232, out_features=1, bias=True)
      (softmax): Softmax(dim=-1)
    )
  )
)>
total parameters: 432135662
2021-08-23 06:10:21.988018 training from scratch with lr 0.001
2021-08-23 06:10:34.448334 begin..
/opt/conda/envs/GAIN/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: From v0.5, DGLHeteroGraph is merged into DGLGraph. You can safely replace dgl.batch_hetero with dgl.batch
  return warnings.warn(message, category=category, stacklevel=1)
/opt/conda/envs/GAIN/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: From v0.5, DGLHeteroGraph is merged into DGLGraph. You can safely replace dgl.unbatch_hetero with dgl.unbatch
  return warnings.warn(message, category=category, stacklevel=1)
/opt/conda/envs/GAIN/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2021-08-23 06:11:21.841285 | epoch  1 | step   20 |  ms/b 2369.64 | train loss 66.369 | NA acc: 0.94 | not NA acc: 0.00  | tot acc: 0.75 
2021-08-23 06:12:00.457237 | epoch  1 | step   40 |  ms/b 1930.79 | train loss 7.077 | NA acc: 0.97 | not NA acc: 0.00  | tot acc: 0.78 
2021-08-23 06:12:34.453233 | epoch  1 | step   60 |  ms/b 1699.79 | train loss 6.927 | NA acc: 0.98 | not NA acc: 0.00  | tot acc: 0.78 
2021-08-23 06:13:12.267968 | epoch  1 | step   80 |  ms/b 1890.73 | train loss 6.768 | NA acc: 0.98 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:13:51.407032 | epoch  1 | step  100 |  ms/b 1956.95 | train loss 6.525 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:14:24.943341 | epoch  1 | step  120 |  ms/b 1676.81 | train loss 6.696 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.79 
2021-08-23 06:15:03.476184 | epoch  1 | step  140 |  ms/b 1926.64 | train loss 7.117 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:15:42.035104 | epoch  1 | step  160 |  ms/b 1927.94 | train loss 6.840 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:16:21.539570 | epoch  1 | step  180 |  ms/b 1975.22 | train loss 6.663 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:17:00.399196 | epoch  1 | step  200 |  ms/b 1942.97 | train loss 6.519 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:17:31.990054 | epoch  1 | step  220 |  ms/b 1579.54 | train loss 6.537 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:18:08.583272 | epoch  1 | step  240 |  ms/b 1829.65 | train loss 6.423 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:18:48.850216 | epoch  1 | step  260 |  ms/b 2013.34 | train loss 6.350 | NA acc: 0.99 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:19:26.856756 | epoch  1 | step  280 |  ms/b 1900.32 | train loss 6.184 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:20:04.076726 | epoch  1 | step  300 |  ms/b 1860.99 | train loss 6.287 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:20:41.283108 | epoch  1 | step  320 |  ms/b 1860.31 | train loss 6.080 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:21:18.315919 | epoch  1 | step  340 |  ms/b 1851.63 | train loss 6.009 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:21:56.772061 | epoch  1 | step  360 |  ms/b 1922.80 | train loss 5.895 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:22:32.747201 | epoch  1 | step  380 |  ms/b 1798.75 | train loss 6.157 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
2021-08-23 06:23:09.302038 | epoch  1 | step  400 |  ms/b 1827.74 | train loss 5.940 | NA acc: 1.00 | not NA acc: 0.00  | tot acc: 0.80 
